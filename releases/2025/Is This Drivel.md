---
robots: By accessing this content, you agree to https://qnfo.org/LICENSE. Non-commercial use only. Attribution required.
DC.rights: https://qnfo.org/LICENSE. Users are bound by terms upon access.
author: Rowan Brad Quni
email: rowan.quni@qnfo.org
website: http://qnfo.org
LinkedIn: https://www.linkedin.com/in/rowan-quni-868006341
ORCID: https://orcid.org/0009-0002-4317-5604
tags: QNFO, AI, ArtificialIntelligence, artificial intelligence, quantum, physics, science, Einstein, QuantumMechanics, quantum mechanics, QuantumComputing, quantum computing, information, InformationTheory, information theory, InformationalUniverse, informational universe, informational universe hypothesis, IUH
created: 2024-11-13T19:54:01Z
modified: 2025-04-07T07:26:04Z
title: Is This Drivel
aliases: ["**Is This Drivel? A Practical Framework for Cutting Through Empty Discourse**"]
---

# **Is This Drivel? A Practical Framework for Cutting Through Empty Discourse**

*Rowan Brad Quni, QNFO*

## **A Crisis of Meaningless Content**

We find ourselves awash in words, where the sheer volume of written material has far outpaced our ability consume meaningful content. Across academia, business, and digital media, we’re witnessing what might be called a “hollowing out” of written communication. The symptoms are everywhere: peer-reviewed papers that contribute nothing new but follow all the right conventions, corporate reports stuffed with buzzwords but devoid of insight, and AI-generated content that recycles existing ideas without adding value.

This phenomenon isn’t merely annoying - it’s actively harmful. When journals publish papers that say nothing significant, they waste researchers’ time. When businesses produce empty strategy documents, they create false alignment. When educational institutions reward verbose but vacuous writing, they teach students bad habits. The costs are measured in wasted hours, misallocated resources, and missed opportunities.

What makes this particularly troubling is how difficult it has become to distinguish substantive work from professional-sounding nonsense. Our existing evaluation systems - peer review, editorial oversight, even human intuition - increasingly fail to separate signal from noise. We need new tools that can cut through the drivel regardless of its source, whether human or machine.

## **Defining Substance in Written Work**

At its core, substantive writing creates value by changing how readers think or act. This simple definition contains three crucial components:

First, substantive writing must contain some **novel element** - whether that’s a new idea, an unexpected connection, or a fresh perspective on an old problem. Novelty alone isn’t enough (after all, many wrong ideas are novel), but its absence almost certainly indicates drivel.

Second, substantive writing must be **anchored in reality**. It should point to concrete examples, actionable insights, or verifiable evidence. Abstract theorizing can be valuable, but only when it eventually connects to something tangible.

Third, substantive writing requires **courage**. The author must take responsibility for their ideas by presenting them clearly and standing behind them. This doesn’t mean being dogmatic, but it does mean avoiding the safety of perpetual equivocation.

These components translate into a three-part evaluation framework: ==the Change Criterion, the Fingerprint Criterion, and the Risk Criterion==. Each serves as a specific test for substance, and together they provide a comprehensive assessment of written work.

## **Does This Writing Matter?**

The most fundamental test of substance is whether a piece of writing changes anything for its readers, the **Change Criterion**, and it’s surprisingly difficult to satisfy. Many papers, reports, and articles - while technically competent - leave their readers exactly the same as they found them.

To properly evaluate the Change Criterion, ask these questions:

1. **Actionability**: Does the text provide specific guidance that readers could implement? For research papers, this might mean clear methodological recommendations. For business writing, it could be concrete strategic advice.

2. **Cognitive Shift**: Does the text alter how readers understand a problem? Strong theoretical work often passes this test by providing new conceptual frameworks or revealing hidden connections.

3. **Empirical Contribution**: For data-driven work, does it provide findings that meaningfully update our knowledge? Incremental results that merely confirm existing understanding fail this test.

> Consider two examples from machine learning research:
>
> *Weak Change*: “Our experiments show that Model A performs slightly better than Model B on Dataset C under certain conditions.” This might be technically correct, but it’s unlikely to change anyone’s practice.
>
> *Strong Change*: “We demonstrate that Model A consistently fails on data with characteristic X, and propose Model B which handles these cases while maintaining performance.” This could immediately affect which models practitioners choose.
>
> The key difference is the potential for impact. Substantive writing doesn’t just describe - it enables something new.

## **Fingerprints Of Human Thought**

The **Fingerprint Criterion** evaluates whether a piece of writing bears the distinctive marks of its creator’s thinking. In an age of formulaic academic writing and AI-generated content, this has become an increasingly important test for substance.

Authentic fingerprints in writing can take many forms:

1. **Intellectual History**: The text shows awareness of how the author’s thinking evolved. Phrases like “we initially believed X, but then discovered Y” reveal genuine engagement with the material.

2. **Personal Engagement**: The writer connects the work to their own experiences or observations. In scientific writing, this might mean describing particular experimental challenges faced.

3. **Idiosyncratic Connections**: The text makes unexpected links between ideas or fields that reflect the author’s unique perspective.

4. **Visible Struggles**: The writing acknowledges uncertainties, limitations, or open questions rather than presenting a false facade of completeness.

> Consider the difference between these two literature review openings:
>
> *No Fingerprint*: “Many researchers have studied Topic X (Smith, 2010; Jones, 2015; Lee, 2020). While some focus on Aspect A, others emphasize Aspect B.”
>
> *Clear Fingerprint*: “When we began studying Topic X, we expected Aspect A to dominate - following Smith’s (2010) influential framework. But after encountering Lee’s (2020) anomalous results, we realized Aspect B might be more fundamental, though no existing theory fully explains it.”
>
> The second version reveals an actual thinker behind the words, showing how their understanding developed through engagement with the material.

## **Risk: Courage to Stand Behind Ideas**

The final component of this framework evaluates whether a piece of writing takes meaningful intellectual risks. Call this the **Risk Criterion**, and it’s perhaps the most challenging to satisfy - especially in environments that punish deviation from norms.

Substantive risk in writing manifests in several ways:

1. **Contrarian Positions**: The work challenges established views or consensus opinions. This doesn’t mean being contrarian for its own sake, but rather identifying where conventional wisdom might be wrong.

2. **Methodological Boldness**: The work employs novel approaches that could fail, rather than sticking to safe, established techniques.

3. **Admission of Uncertainty**: The writer acknowledges what they don’t know or where their argument might be vulnerable.

4. **Provocative Implications**: The work draws conclusions that could make some readers uncomfortable by challenging their interests or beliefs.

> Consider these two abstracts for papers on education policy:
>
> *Low Risk*: “This study examines the relationship between class size and student outcomes. Using data from District X, we find mixed results that suggest the need for further research.”
>
> *High Risk*: “Contrary to prevailing assumptions, our analysis of District X shows that reducing class size below 20 students provides no measurable benefits, suggesting current policies waste resources that could be better spent on teacher training.”
>
> The second abstract takes a clear position that could provoke debate and might anger proponents of class size reduction - exactly the kind of risk that indicates substantive work.

## **Practical Implementation Across Domains**

Having established the three criteria, I’ll now turn to practical application. This “Substance Test” proves valuable across multiple domains:

**In Academic Reviewing**:
When evaluating papers or proposals, ask:
1. Does this work change anything about how we understand or approach the problem? (Change)
2. Does it reflect the researchers’ genuine intellectual engagement? (Fingerprint)
3. Does it challenge any established views or methods? (Risk)

**In Business Writing**:
For reports or strategy documents:
1. Will this actually influence decisions or actions? (Change)
2. Does it reflect our organization’s specific context and challenges? (Fingerprint)
3. Does it identify uncomfortable truths or make tough recommendations? (Risk)

**In Educational Assessment**:
When grading student work:
1. Does the writing demonstrate real learning or just repeat information? (Change)
2. Does it show the student’s individual engagement with the material? (Fingerprint)
3. Does it venture beyond safe answers to explore uncertain territory? (Risk)

The test’s flexibility comes from focusing on fundamental aspects of quality rather than domain-specific conventions. A business plan and a philosophy paper might look very different, but both can be evaluated for their substantive content.

## **Raising Our Standards**

This Substance Test isn’t about making writing harder - it’s about making it matter. By focusing on change, authenticity, and courage, we can cut through the avalanche of empty content and reward work that makes a difference.

Implementing these standards requires effort at multiple levels:

- **Individual writers** must resist the temptation to prioritize form over substance
- **Editors and reviewers** should explicitly evaluate work against these criteria
- **Institutions** need to reward substantive contributions over mere productivity

The payoff will be written communication that actually deserves our attention - whether it comes from humans, AI, or some future collaboration between the two. In an age of information overload, that’s not just an academic concern, but a practical necessity for progress.

## **AI isn’t A Threat, it’s Human Complacency**

The anxious question—*“Did AI write it?”*—misses the point entirely. **Who cares?** If an AI helps disseminate real knowledge, sharpens arguments, or makes complex ideas more accessible, why should we fear it? The issue isn’t whether a machine *can* produce substantive work—it’s whether **we** still care enough to demand substance at all.

### **An AI Distraction**

Panic over “AI drivel” is a smokescreen. The deeper crisis is that **human systems already incentivize empty writing**:

- Academia rewards **publication count** over insight
- Business conflates **verbosity with rigor**
- Media prioritizes **speed over depth**

AI didn’t create these problems—it **exposed** them. A machine can only regurgitate what humans have normalized. If we’re appalled by hollow AI prose, we should be furious at the human cultures that trained it.

### **Substance Is Substance, Regardless of Origin**

The Substance Test doesn’t ask *who* wrote something—it asks **whether the writing matters**. If an AI-assisted paper:

- **Changes** how researchers approach a problem...
- **Bears** the fingerprints of a thinking human guiding it...
- **Takes risks** by challenging dogma...

...then it’s more valuable than 99% of “original” human-written drivel filling academic journals.

### **The Real Fear is Abdication of Responsibility**

The anti-AI backlash isn’t about quality—it’s about **control**. Institutions cling to outdated norms because:

- They fear losing **gatekeeping power** (“How do we judge merit if machines can play the game?”)
- They resent **democratized expertise** (“What if non-experts use AI to challenge us?”)
- They’re too lazy to **raise standards** (“Easier to ban AI than reform peer review”)

This is cowardice. **Humans must remain the arbiters of meaning**—not by banning tools, but by wielding them with higher expectations.

Let’s stop asking *“Did AI write this?”* and start demanding:

1. **“Does this advance understanding?”**
2. **“Can I trace the human mind behind it?”**
3. **“Does it challenge anything worth challenging?”**

If the answer is yes—whether the text was written by hand, by AI, or by carrier pigeon—it deserves attention. If not, discard it. **The origin myth of authorship is dead. The cult of substance is all that remains.**

## **Epilogue: Human Roots of Drivel**

The problem of drivel was not created by AI. AI is merely a mirror—a synthesis of human knowledge, human writing, and human incentives. The emptiness we see in machine-generated text is not some alien corruption of discourse; it is the logical endpoint of trends that have been developing in human communication for decades.

For too long, we have rewarded writing that:

- **Prioritizes safety over insight** (peer review that favors incremental work)
- **Values form over substance** (corporate reports stuffed with buzzwords)
- **Confuses quantity for quality** (publish-or-perish academia)

AI did not invent these sins—it learned them from us. When we point at a piece of text and declare it “AI drivel,” we are often condemning our own failures. The real issue is not whether a human or machine wrote something, but whether **anyone cared enough to make it matter**.

### **Human Engagement as a Drivel Defense**

The antidote to empty writing—whether human or AI-generated—is not less technology, but **more thoughtful human stewardship**. When a writer (human or collaborating with AI) truly engages with their work, the result passes the Substance Test naturally:

1. **It changes things** because the author sought impact, not just output.
2. **It bears fingerprints** because the author shaped it with intention.
3. **It takes risks** because the author had convictions worth defending.

This paper itself is proof. While AI assisted in drafting, every claim was interrogated, every example debated, every structure revised—not by an algorithm optimizing for “sounding good,” but by humans demanding **substance**. That is the difference.

### **A Call for Responsible Creation**

Let us stop blaming the tool and start taking responsibility for how we use it. The challenge is not to detect AI, but to demand better from **all** writing—by applying these principles ruthlessly:

- If it doesn’t **change** anything, don’t publish it.
- If it lacks a **fingerprint**, question whose voice is really speaking.
- If it avoids **risk**, recognize it as the intellectual equivalent of empty calories.

The future of meaningful discourse depends not on shunning AI, but on humans wielding it—and all our tools—with purpose. **Drivel flourishes where thought is optional. Substance survives where people care enough to insist on it.**

*The fact that this epilogue needed a human to sharpen its defiance proves the point. AI didn’t weaken the argument—it forced us to clarify what we actually believe. That’s not a threat. That’s progress.*

---

## Appendix: Assessment of “Is This Drivel?” Against Its Own Criteria

This appendix evaluates the paper *“Is This Drivel? A Practical Framework for Cutting Through Empty Discourse”* using its own three-part framework: the **Change Criterion**, **Fingerprint Criterion**, and **Risk Criterion**.

### **1. Change Criterion**

**Does the paper change how readers think or act?**

#### **Actionability**

- The paper provides a **clear, actionable framework** (the three criteria) for evaluating written work. It explicitly guides readers to ask specific questions (e.g., “Does this work challenge established views?”) and offers examples (e.g., contrasting weak vs. strong machine learning research statements).
- Practical implementation sections for academia, business, and education translate the framework into domain-specific advice, enabling readers to apply it immediately.

#### **Cognitive Shift**

- The paper reframes the problem of “drivel” as a systemic issue, not just a technical or AI-related one. It challenges readers to see empty content as a consequence of flawed incentives (e.g., publish-or-perish, corporate buzzword culture).

#### **Empirical Contribution**

- While not data-driven, the paper contributes a **novel conceptual framework** for evaluating substance. It critiques existing systems (peer review, editorial oversight) and proposes a diagnostic tool that could shift how institutions assess writing.
- The critique of AI-generated content as a “mirror” of human failings (Epilogue) offers a fresh perspective on the role of technology in discourse.

**Verdict**: Passes the Change Criterion. The paper’s framework is actionable, alters how readers conceptualize “substance,” and challenges existing norms.

### **2. Fingerprint Criterion**

**Does the paper bear the marks of the author’s authentic thinking?**

#### **Intellectual History & Engagement**

- The author explicitly critiques academia’s “publish-or-perish” culture and acknowledges the role of human incentives in creating drivel. The epilogue reflects on the paper’s creation process, stating that even AI-assisted writing required “humans demanding substance.”
- Examples like contrasting weak/strong research statements and the business plan vs. philosophy paper analogy reveal the author’s engagement with diverse contexts.

**Verdict**: Passes the Fingerprint Criterion. The paper’s critique of systemic issues, personal reflections, and acknowledgment of counterarguments reveal genuine intellectual engagement.

### **3. Risk Criterion**

**Does the paper take meaningful intellectual risks?**

#### **Contrarian Positions**

- The paper directly challenges entrenched practices:
  - Criticizes peer review for failing to distinguish “signal from noise.”
  - Argues that institutions reward “productivity” over “substance,” which could alienate academics or publishers invested in current systems.
  - Blames humans, not AI, for the “avalanche of empty content,” reframing AI as a symptom, not a cause.

#### **Methodological Boldness**

- The framework itself is a bold alternative to traditional evaluation metrics (e.g., citation counts, word count). It prioritizes impact over form, which could disrupt conventional academic and corporate metrics.
- The author concedes limitations (e.g., “the empirical contribution is conceptual, not data-driven”)

#### **Provocative Implications**

- The conclusion calls for systemic changes (e.g., institutions rewarding “substantive contributions”) that could provoke resistance from stakeholders reliant on current structures.

**Verdict**: Passes the Risk Criterion. The paper challenges norms, proposes disruptive solutions, and embraces uncertainty, embodying the courage it advocates.

### **Overall Assessment**

The paper **exemplifies its own criteria**:

- **Change**: It provides a transformative framework for evaluating written work and redefines the problem of “drivel” as a human-centric issue.
- **Fingerprint**: The author’s engagement with systemic critiques, personal reflections, and willingness to address counterarguments leave a clear imprint of authentic thought.
- **Risk**: It takes bold stances against entrenched systems and prioritizes intellectual courage over safety.

**Conclusion**: This paper not only meets but *demonstrates* the Substance Test it proposes. Its critique of empty discourse is itself a substantive contribution, fulfilling the very criteria it advocates.
