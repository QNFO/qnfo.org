---
author: Rowan Brad Quni
email: rowan.quni@qnfo.org
website: http://qnfo.org
ORCID: https://orcid.org/0009-0002-4317-5604
robots: By accessing this content, you agree to https://qnfo.org/LICENSE. Non-commercial use only. Attribution required.
DC.rights: https://qnfo.org/LICENSE. Users are bound by terms upon access.
created: 2025-04-12T09:52:06Z
modified: 2025-04-12T14:09:18Z
title: 1-1 Introduction
aliases: [Modern Physics Metrology]
---

# [[releases/2025/Modern Physics Metrology/Modern Physics Metrology|Modern Physics Metrology]]

# Part 1, Section 1: Introduction

**Foundational Flaws and Self-Reference**

Science progresses by building upon previous knowledge, constructing elaborate theoretical edifices on foundations assumed to be solid. Physics, in particular, relies on precise measurement and rigorous mathematical description, striving for an objective account of reality. However, this work argues that the very foundations upon which much of modern fundamental physics and cosmology are built–the systems of measurement (SI units) and the conventional mathematical frameworks employed–contain critical flaws and anthropocentric biases. More alarmingly, recent developments in metrology, specifically the fixing of fundamental constants like the speed of light (c) and Planck’s constant (h) by definition, have created a **closed, self-referential system**. This system, built upon potentially flawed premises tracing back to historical contingencies and mathematical idealizations (like base-10 and assumed linearity), premises which are then implicitly locked in by these metrological definitions, risks becoming **dangerously detached from empirical falsification** regarding its own foundational assumptions.

The core thesis of this work is that this self-referential loop, built upon potentially flawed premises tracing back to historical contingencies and mathematical idealizations, constitutes a **house of cards**. By fixing constants derived from potentially incomplete or misinterpreted theories (like quantum mechanics based on inherent quantization, or special relativity assuming constant $c$for all phenomena), the system creates **artificial dependencies**. Measurements are no longer independent probes of reality but become exercises in realizing units defined by these fixed constants. Theoretical models are then built using these potentially artifact-laden constants and mathematical tools (like base-10 decimals or standard calculus applied to intrinsically geometric phenomena). When discrepancies inevitably arise between these models and observation (e.g., the need for dark matter and dark energy), they are interpreted not as evidence falsifying the faulty foundations, but as requiring new, ad-hoc entities within the established, self-consistent but potentially incorrect, framework.

This work undertakes a **systematic, logical deconstruction** of this edifice. We will conduct a forensic analysis, starting with the anthropocentric and potentially inadequate nature of our conventional number systems and geometry (Part 1). We will contrast this with the potential for a more fundamental description based on natural geometric constants like π and φ (Part 1). We will then meticulously dissect the modern SI system, exposing the **self-referential loops created by fixing $c$and $h$** and demonstrating how this enshrines potentially flawed assumptions, like inherent quantization originating from Planck’s “mathematical trick,” into the very definition of our measurement standards (Part 2). Finally, we will **keep score**, demonstrating how these foundational flaws and measurement artifacts **propagate through the system**, arguing that they provide a compelling, parsimonious explanation for the apparent necessity of the ≈95% “dark sector” (dark matter and dark energy) in standard cosmology (Part 3). The aim is to demonstrate, through rigorous analysis and exposure of internal inconsistencies and questionable dependencies, that the “dark universe” is likely an **unavoidable consequence of measuring reality with a flawed, self-referential toolkit**, thereby falsifying the need for these exotic entities.

This analysis is intended to be irrefutable within its logical structure, showing how reliance on potentially incorrect foundational assumptions, enshrined by definitional fiat in our measurement system, inevitably leads to the paradoxes and unexplained phenomena that currently plague fundamental physics. By exposing the fragility of this “house of cards,” this work motivates the urgent need for developing and exploring alternative descriptive frameworks, such as those grounded in continuum principles and natural geometric constants, capable of providing a more direct, less artifact-laden engagement with physical reality.

---

**[[releases/2025/Modern Physics Metrology/1-2 Counting Fingers|1-2 Counting Fingers]]**
