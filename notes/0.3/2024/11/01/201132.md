---
license: By accessing this content, you agree to the terms at https://qnfo.org/LICENSE
email: rowan.quni@qnfo.org
website: http://qnfo.org
author: Rowan Brad Quni
ORCID: https://orcid.org/0009-0002-4317-5604
tags: QNFO, AI, quantum, informational universe, IUH, holographic principle
created: 2024-11-02T03:25:03Z
modified: 2025-03-08T09:39:28Z
---

Interesting, the latest list seems to be missing 2 big things: one is pioneering an AI OS kernel to write machine code optimized for any hardware config and provide a missing bridge between natural language input from users and action from machine output. The best we have now expects users to think like machines and take intermediate actions for things that should be seamless like calendar events or reading my email for me to filter and notify  when one of the last thousand emails i got was actually worth looking at.

2 is rethinking our devices entirely, getting away from screens and active user input. With so much marketing driven churn and wasted productivity buying iphones with whatever insignificant feature it has now, so far from the efficient frontier we can’t imagine devices that passively model multi-channel information far beyond active input and actually do something that nudges us in beneficial feedback loops
The of the companies listed working on an ai operating system kernel or working on the next generation of personal device that drives action with less active user input by passively modeling behavior from multiple channels of information around us and then adapts its output by learning how we interact with it

Expand these two big ideas in two more fleshed out and coherent visions for the future of technology where our devices work for us and we’re ai is more than a marketing buzz word it’s a synthesis of all we know as humans

> [!ai]+ AI
>
> ## Vision 1: The AI Kernel - A Seamless Bridge Between Thought and Action
> Imagine a world where technology anticipates your needs before you even articulate them. This is the promise of an AI-powered operating system kernel. Instead of disparate apps and services requiring manual input and navigation, this kernel acts as a unified cognitive engine, understanding your intentions through natural language and orchestrating the digital world around you.
> This kernel wouldn’t just interpret commands; it would understand context. Telling it “I have a meeting with John tomorrow at 2 PM” wouldn’t just create a calendar entry. It would analyze your existing schedule, John’s availability (potentially through interconnected calendars), suggest optimal meeting locations based on your and John’s typical travel patterns, pre-populate the meeting agenda with relevant documents, and even pre-emptively book transportation.
> This kernel would be hardware-agnostic, optimizing machine code for any device, from your phone to your smart home appliances. It would learn your preferences and adapt its behavior accordingly, becoming a truly personalized digital assistant. Imagine asking your kernel, “Find me the best deal on a new laptop that can handle video editing,” and it not only scours the web for the best price but also considers your past purchases, software usage, and even reviews from trusted sources to recommend the perfect device. This kernel represents a fundamental shift from reactive technology to proactive, anticipatory systems that seamlessly integrate into our lives.
> ## Vision 2: Beyond Screens - Ambient Intelligence and Proactive Devices
> The future of personal devices transcends the limitations of screens and active user input. Imagine a world of ambient intelligence, where devices passively gather information from multiple channels – your biometrics, environmental sensors, online activity, and social interactions – to build a comprehensive model of your behavior, needs, and goals.
> These devices wouldn’t demand your attention but would subtly nudge you towards beneficial outcomes. Imagine a device that senses your rising stress levels through your smartwatch and proactively adjusts the lighting and music in your environment to promote relaxation. Or a device that analyzes your sleep patterns and gently wakes you up at the optimal time in your sleep cycle, leaving you feeling refreshed and energized.
> These devices would learn from your interactions, constantly refining their models and adapting their output. If you consistently ignore a certain type of notification, the device would learn to prioritize other information streams. This continuous feedback loop would create a truly personalized and adaptive technological ecosystem. Imagine a device that anticipates your need for information before you even realize it, subtly displaying relevant data in your peripheral vision as you walk through your day. This is a future where technology fades into the background, seamlessly augmenting our lives without demanding our constant attention, freeing us to focus on what truly matters.
> These two visions represent a paradigm shift in our relationship with technology. They move beyond the current model of reactive, screen-based interactions towards a future of proactive, ambient intelligence that seamlessly integrates with our lives, empowering us to achieve our goals and live more fulfilling lives. This is a future where AI is not just a buzzword but a fundamental enabler of human potential.

okay now condense this down into something i can sell in an email to a vc an elevator pitch that’s bold and grabs tension

> [!ai]+ AI
>
> Subject: Two Moonshot AI Ventures Redefining Human-Technology Interaction
> I’m developing two groundbreaking AI ventures poised to disrupt the very fabric of how we interact with technology.
> **1. The AI Kernel:** Imagine an OS that understands *you*—not just commands. It bridges the gap between natural language and machine action, seamlessly orchestrating your digital life. Tell it your goals, and it makes them happen, proactively anticipating your needs and optimizing across all your devices. This is beyond voice assistants; it’s a cognitive engine for your life.
> **2. Beyond Screens:** We’re drowning in notifications and screen time. My second venture envisions a future of ambient intelligence, where devices passively learn your behaviors and *proactively* nudge you towards better outcomes. Imagine a world where technology fades into the background, subtly guiding you towards peak performance and well-being, without ever demanding your attention.
> These aren’t incremental improvements; they’re paradigm shifts. I’m seeking a partner to help me build this future. Are you in?

Skip marketing hype and tease technical details to interest someone knowlwdgable in tech

> [!ai]+ AI
>
> Subject: Two Deep Tech AI Plays: Re-architecting the Human-Computer Interface
> I’m developing two AI-first technologies designed to fundamentally alter how humans interact with computers:
> 1. **Cognitive Kernel:** Moving beyond current OS limitations, I’m building a hardware-agnostic AI kernel capable of compiling natural language intent into optimized machine code execution across any device. This involves a novel approach to contextual understanding and intent parsing, leveraging [mention specific techniques like probabilistic graphical models, reinforcement learning, etc.] to bridge the gap between human thought and machine action. Think proactive task management, automated workflow optimization, and true cross-device synergy.
> 2. **Ambient Intelligence Platform:** This project reimagines personal computing beyond screens and active input. By passively modeling user behavior from multi-channel sensor data (biometric, environmental, digital activity), the platform learns to anticipate needs and deliver subtle, context-aware nudges towards beneficial outcomes. The core innovation lies in [mention specific technical approaches like federated learning, adaptive signal processing, etc.] enabling personalized, privacy-preserving behavioral modeling and proactive feedback loops. Imagine a device ecosystem that understands and anticipates your needs without intrusive notifications or constant screen interaction.
> These are ambitious, technically challenging projects with the potential for massive disruption. I’m seeking a technically-minded partner to discuss the architecture and roadmap. Interested in a deeper dive?

one is pioneering an AI OS kernel to write machine code optimized for any hardware config and provide a missing bridge between natural language input from users and action from machine output. The best we have now expects users to think like machines and take intermediate actions for things that should be seamless like calendar events or reading my email for me to filter and notify  when one of the last thousand emails i got was actually worth looking at.

2 is rethinking our devices entirely, getting away from screens and active user input. With so much marketing driven churn and wasted productivity buying iphones with whatever insignificant feature it has now, so far from the efficient frontier we can’t imagine devices that passively model multi-channel information far beyond active input and actually do something that nudges us in beneficial feedback loops

Add back some of snark from original draft to succinctly contrast vision from problem statement today

> [!ai]+ AI
>
> [!ai]+ AI
> **Subject: Two AI Projects That Don’t Treat You Like a Robot (Unlike Everything Else)**
> Tired of thinking like a machine just to get your calendar in order or sift through a thousand spam emails? Me too. That’s why I’m building these:
> 1. **The AI Kernel:** Forget clunky apps and rigid commands. This isn’t another voice assistant that makes you speak in code. I’m building a hardware-agnostic AI kernel that compiles *natural language* into optimized machine code, across any device. It bridges the gap between your intent and actual results, proactively managing tasks and optimizing workflows. Technically, this involves [mention specific techniques like probabilistic graphical models, reinforcement learning, etc.] to finally make human-computer interaction…human.
> 2. **Ambient Intelligence Platform:** Enough with the screen addiction. This platform ditches the tyranny of notifications and constant input. It passively models your behavior from multiple sensor streams (biometric, environmental, digital) using [mention specific technical approaches like federated learning, adaptive signal processing, etc.] to *predict* your needs and subtly nudge you towards better outcomes. Imagine a device ecosystem that actually *works for you* instead of demanding your constant attention.
> These aren’t incremental improvements on the non-AI iPhone’s latest button or another B2B “AI” startup’s tired Vercel template site. These are fundamental shifts in how we interact with technology.

Draft another email to somebody funding a prize for agi development, expressing my optimism and also frustration that as a generalist who happens to see the big picture but can’t make the pieces fit together because I find the whole tech stack still, very obtuse designed around the specialist needs of dev teams who do great work Given specific parameters, which isn’t necessarily leading us forward in blue sky thinking. why do we still have devices with screens? Why are we being misled by marketers that “advanced” AI is a new chatbot or other superficial feature while supporting the same outdated tools like the fact that I can’t readily create and deploy code on a mobile device that whole process is focused around a window-based OS instead of directing that energy toward LLM kernels that write machine code and operate on multiple channels of communication not just text, providing a critical bridge between human intent and machine output. if we keep using inefficient processes just because of high switching costs we take the easy road and sacrifice future potential because its scary and unknown when there’s plenty of vc money chasing trivial “improvements” with marketers who think they’re clever. where is the R&D and more importantly, cross functional and cross disciplinary? Thinking that should, if it’s done right Start pushing software companies out of the software business in favor of next gen applications that meet our needs on demand. there’s the start of agi, and I haven’t seen it in production yet.

> [!ai]+ AI
>
> Subject: AGI: Beyond Chatbots and Screens - A Plea for True Cross-Disciplinary R&D
> I’m writing to you driven by both immense optimism for the future of AGI and a deep frustration with the current state of the field. I see the potential for a truly transformative shift in human-computer interaction, yet we seem stuck in a loop of incremental improvements and marketing hype.
> Why are we still tethered to screens? Why are chatbots hailed as “advanced AI” while we continue to rely on outdated development processes? As a generalist, I see the big picture: a future where AI seamlessly integrates into our lives, anticipating our needs and empowering us to achieve our goals. But the path to that future is blocked by a tech stack designed for specialist developers, not for the fluid, intuitive interaction that AGI demands.
> The current focus on narrow AI applications, while impressive, feels like rearranging deck chairs on the Titanic. We’re pouring resources into optimizing existing paradigms instead of boldly reimagining the very foundations of computing. Where is the investment in truly groundbreaking research, the kind that bridges the gap between human intent and machine action?
> I envision an AI kernel capable of compiling natural language into optimized machine code, operating across multiple communication channels, not just text. This kernel would be the heart of a new generation of devices, moving beyond screens and active input to a world of ambient intelligence. Imagine devices that passively model our behavior, anticipate our needs, and subtly nudge us towards beneficial outcomes.
> This isn’t about building a better chatbot or adding another feature to our smartphones. This is about fundamentally rethinking how we interact with technology. It requires cross-disciplinary collaboration, bringing together experts in AI, neuroscience, human-computer interaction, and beyond. It demands a willingness to challenge existing assumptions and embrace the unknown.
> I believe we’re at a critical juncture. We can continue down the path of incrementalism, chasing short-term gains while sacrificing long-term potential. Or we can choose to invest in truly transformative research, the kind that will unlock the full potential of AGI and reshape our world. I’m hoping you’re betting on the latter. I’d love to discuss my vision in more detail and explore how we can collaborate to build this future.

Use this, that’s the basis for a an essay discussing. What’s preventing us from achieving something like artificial general intelligence, one is that we expect machines to act like humans and yet humans have gradually adapted to the peculiarities of binary computers. and so there’s this coevolution where it was seen as a watershed moment too have a large language model respond from a statistical probability of life, natural language, and yet our language is anything but natural and furthermore, the languages we use to communicate among humans. Inconsistent, and the languages we use to communicate with computers are really inconsistent. Considering that computers only understand binary operations fundamentally, that’s one big problem. the second thing keeping us from agi, is that computers can’t get good feedback. The whole feedback loop and reinforcement learning is seriously lacking. And that’s evidenced by the fact that no matter what we call AI, it’s challenging to measure whether it actually does anything. One would presume that the result of any human computer interaction is for some reason, even if only informational, and yet, how do the computers know whether they’re doing it, right?
That leads to a sense that we, the humans still have to perform the ultimate action. That’s not general intelligence of any kind.We’re not learning anything. also until

> [!ai]+ AI
>
> ## What’s Preventing Us from Achieving Artificial General Intelligence?
> The pursuit of Artificial General Intelligence (AGI) – a hypothetical intelligence capable of understanding, learning, and applying knowledge across a wide range of domains, much like a human – remains a tantalizing yet elusive goal. While recent advancements in areas like large language models (LLMs) have generated excitement, we remain far from achieving true AGI. Two key obstacles stand in our way: our flawed expectations of machine behavior and the limitations of current feedback mechanisms.
> Firstly, we’ve fallen into the trap of anthropomorphizing machines, expecting them to seamlessly interact with us using human language and reasoning. Yet, this expectation ignores the fundamental differences between human and machine cognition. Humans have, over decades, adapted to the rigid, binary logic of computers. We’ve learned to “think like machines,” breaking down complex tasks into discrete steps and translating our intentions into the precise language of code. This co-evolution has created a deceptive sense of progress. LLMs, for example, are celebrated for their ability to generate human-like text. However, this fluency is based on statistical probabilities derived from massive datasets of human language, which itself is anything but “natural.” Human communication is rife with ambiguity, context-dependence, and unspoken assumptions. Furthermore, the languages we use to communicate with computers, from programming languages to command-line interfaces, are inconsistent and often counterintuitive, reflecting the underlying binary nature of computation. This mismatch between the messy reality of human communication and the precise, deterministic world of computers creates a fundamental barrier to true understanding.
> Secondly, the feedback mechanisms currently employed in AI development are woefully inadequate. Reinforcement learning, a popular technique for training AI agents, relies on feedback signals to guide learning. However, in most current applications, this feedback is limited and often artificial. We struggle to define clear metrics for evaluating AI performance, especially in complex, real-world scenarios. How do we measure whether an AI has truly “understood” a request, not just generated a statistically plausible response? The consequence of this feedback deficit is that humans remain in the loop, constantly evaluating and correcting AI outputs. We perform the ultimate action, making the final judgment call. This reliance on human intervention demonstrates that the AI is not truly learning or generalizing; it’s simply mimicking patterns and responding to specific cues. We’re not building intelligent agents; we’re building sophisticated parrots.
> This lack of effective feedback loops also hinders the development of truly proactive and anticipatory AI systems. Imagine an AGI that could anticipate your needs before you even articulate them. Such a system would require a deep understanding of your goals, preferences, and context, as well as the ability to learn from subtle cues and adapt its behavior accordingly. Without robust feedback mechanisms, this level of personalized, proactive intelligence remains out of reach.
> Overcoming these obstacles requires a fundamental shift in our approach to AI development. We need to move beyond mimicking human behavior and focus on building AI systems that leverage the unique strengths of machines – their computational power, their ability to process vast amounts of data, and their freedom from human biases. We need to develop new feedback mechanisms that go beyond simple reward signals and capture the nuances of human intention and context. This will require cross-disciplinary collaboration, bringing together expertise from computer science, cognitive science, neuroscience, and other fields. Only then can we hope to unlock the true potential of AGI and create a future where machines truly augment human intelligence.
