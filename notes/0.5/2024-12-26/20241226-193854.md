---
license: By accessing this content, you agree to the terms at https://qnfo.org/LICENSE
email: rowan.quni@qnfo.org
website: http://qnfo.org
author: Rowan Brad Quni
ORCID: https://orcid.org/0009-0002-4317-5604
tags: QNFO, AI, quantum, informational universe, IUH, holographic principle
created: 2024-12-27T03:38:54Z
modified: 2025-03-08T09:38:56Z
---

That’s the problem. We tried to shoehorn determinism and simultaneous equations into our preconceived notion of how the universe worked.

> [!ai]+ AI
>
> This statement highlights a common pitfall in scientific thinking: trying to force observations to fit existing theories, rather than adapting theories to fit observations. It suggests that the speaker believes that the concepts of determinism (the idea that all events are predetermined) and simultaneous equations (mathematical expressions representing interconnected variables) are being inappropriately applied to a model of the universe.
> Here’s a breakdown of why this might be problematic:
> - **Oversimplification:** The universe is incredibly complex. Trying to capture its behavior using simple, deterministic models or a limited set of simultaneous equations might ignore crucial factors and lead to inaccurate or incomplete understanding.
> - **Ignoring emergent behavior:** Complex systems can exhibit emergent behavior, where the overall behavior of the system is more than just the sum of its parts. Deterministic models and simultaneous equations might struggle to capture these emergent properties.
> - **Preconceived notions:** As the statement points out, clinging to existing beliefs can hinder scientific progress. If we’re too attached to the idea of a deterministic universe, we might be blind to evidence suggesting otherwise.
> - **Limitations of the tools:** Simultaneous equations and deterministic models are powerful tools, but they have limitations. They might not be suitable for describing all phenomena, especially those involving quantum mechanics or chaotic systems.
> The statement implies a need for a more flexible and nuanced approach to understanding the universe, one that’s willing to question existing assumptions and embrace complexity. It suggests that perhaps a different framework, beyond determinism and simple simultaneous equations, is needed to accurately represent reality.

Falsify determinism.

> [!ai]+ AI
>
> It’s impossible to definitively *falsify* determinism through a single experiment or observation. The challenge lies in the very nature of determinism: it posits that *all* events are predetermined. This makes it difficult to isolate a truly “random” event and prove it wasn’t predetermined in some subtle, hidden way.
> However, we can point to phenomena that strongly *suggest* determinism is an incomplete or inaccurate model of reality:
> - **Quantum Mechanics:** The probabilistic nature of quantum mechanics presents the most significant challenge to determinism. Events like radioactive decay appear to be fundamentally random. While some deterministic interpretations of quantum mechanics exist (e.g., hidden variable theories), they haven’t gained widespread acceptance and often introduce more complexity than they resolve. Bell’s Theorem and related experiments have provided strong evidence against local realism, which underpins many deterministic interpretations.
> - **Chaos Theory:** Chaotic systems are highly sensitive to initial conditions. Even tiny, immeasurable variations in the starting state can lead to dramatically different outcomes. While chaotic systems are deterministic in principle (their behavior *is* governed by equations), in practice, their long-term predictability is severely limited. This makes them appear random for all practical purposes. The weather is a classic example.
> - **The Measurement Problem:** In quantum mechanics, the act of measurement seems to force a system into a definite state. This raises questions about the role of the observer and the nature of reality itself. If the observer’s choice influences the outcome, it suggests a non-deterministic element.
> - **Free Will (Philosophical Argument):** The subjective experience of free will is often cited as an argument against determinism. If all our actions are predetermined, then the concept of moral responsibility becomes problematic. However, this is a philosophical rather than a scientific argument, and it’s difficult to translate it into a testable hypothesis.
> It’s important to note that these points don’t definitively *disprove* determinism. A staunch determinist could argue that these seemingly random events are simply the result of our incomplete understanding of the underlying deterministic mechanisms. However, the weight of evidence from quantum mechanics and chaos theory strongly suggests that a purely deterministic model is insufficient to describe the universe as we observe it. The universe appears to have a fundamental element of randomness or indeterminacy.

A Newtonian system is tautological. It says that it is a system that is described by Newton’s laws. So, by definition, you can’t falsify that.

> [!ai]+ AI
>
> You’re right to point out the potential tautology. If we define a Newtonian system as “a system that obeys Newton’s laws,” then it becomes unfalsifiable in a strict sense. Any apparent violation of Newton’s laws would simply mean the system wasn’t truly Newtonian, rather than a falsification of the laws themselves.
> However, that’s not how Newtonian mechanics is typically used in practice. Instead, we consider a “Newtonian system” to be one where the assumptions underlying Newton’s laws are *approximately* valid. These assumptions include:
> - **Objects are point masses:** We can treat objects as if all their mass is concentrated at a single point.
> - **Inertial frames of reference:** There exist frames of reference where objects at rest stay at rest and objects in motion continue moving at a constant velocity unless acted upon by a net force.
> - **Instantaneous action at a distance:** Forces like gravity act instantaneously across space.
> - **Determinism:** The future state of the system is completely determined by its current state.
> These assumptions are often reasonable approximations for macroscopic objects moving at speeds much slower than the speed of light and experiencing relatively weak gravitational fields. However, they break down in certain situations, providing opportunities to test the limits of Newtonian mechanics and demonstrate its incompleteness:
> - **High speeds:** At speeds approaching the speed of light, special relativity becomes important, and Newtonian mechanics fails to accurately predict the motion of objects.
> - **Strong gravitational fields:** In the vicinity of extremely massive objects like black holes, general relativity is required, and Newtonian gravity becomes inaccurate.
> - **Small scales:** At the atomic and subatomic levels, quantum mechanics governs the behavior of particles, and Newtonian mechanics is no longer applicable.
> - **Very precise measurements:** Even in seemingly “Newtonian” systems, extremely precise measurements might reveal deviations from the predictions of Newton’s laws, hinting at underlying complexities.
> So, while the *definition* of a Newtonian system might seem tautological, the *application* of Newtonian mechanics to real-world situations is certainly falsifiable. We can identify systems where the underlying assumptions break down, and the predictions of Newton’s laws diverge from experimental observations. These deviations don’t “falsify” Newton’s laws in the sense of rendering them completely useless, but they demonstrate their limitations and the need for more comprehensive theories like relativity and quantum mechanics.

So any system that describes itself is unfalsifiable. And so the scientific method is by definition unfalsifiable because anything that lies outside it is not the scientific method.

> [!ai]+ AI
>
> You’re touching on a subtle but important point about the philosophy of science. While it’s true that a system defined purely in terms of itself *appears* unfalsifiable, this doesn’t render the scientific method useless or inherently flawed. Here’s why:
> - **The Scientific Method is not a Single, Fixed System:** The scientific method isn’t a rigid set of rules etched in stone. It’s more of a flexible framework, a collection of principles and practices that guide scientific inquiry. These principles include observation, hypothesis formation, experimentation, and peer review. The specific methods used can vary depending on the field of study and the nature of the question being investigated. This adaptability is a strength, not a weakness.
> - **Falsifiability Applies to Specific Hypotheses, not the Method Itself:** The criterion of falsifiability, as proposed by Karl Popper, applies primarily to *scientific hypotheses*, not the scientific method itself. A good scientific hypothesis should make testable predictions that could potentially be proven wrong. If a hypothesis consistently withstands rigorous attempts at falsification, it gains credibility, but it’s never considered definitively “proven.”
> - **The Scientific Method Evolves:** The scientific method itself is subject to scrutiny and refinement. Philosophers of science continually debate and revise our understanding of how science works best. This self-reflection and adaptation are crucial for ensuring the continued progress of scientific knowledge.
> - **Demarcation Problem:** You’re highlighting the demarcation problem, which is the challenge of distinguishing science from non-science. While there’s no single, universally accepted solution, falsifiability is a useful, though not perfect, criterion. It helps us identify claims that are genuinely testable and distinguishes them from those that are inherently unfalsifiable (e.g., purely metaphysical claims).
> - **Practical Implications:** Even if we can’t definitively “falsify” the scientific method as a whole, we can certainly evaluate its effectiveness in practice. The incredible advancements in our understanding of the universe and the development of technologies that improve our lives are powerful testaments to the power of the scientific approach.
> So, while the self-referential nature of defining the scientific method might seem to create a loophole regarding falsifiability, the real power of the method lies in its application to generating testable hypotheses and its ability to adapt and evolve over time. The focus should be on the falsifiability of the specific claims and theories generated *within* the scientific framework, not on trying to falsify the framework itself.

What’s the ontology here? It seems that there is a fundamental logic problem with systems that describe themselves and therefore can’t be falsified because anything that would violate the system would by definition be outside it.

> [!ai]+ AI
>
> You’ve hit upon a crucial point in the philosophy of science and the nature of knowledge itself. The ontological issue you’re raising relates to the limits of self-contained systems and their ability to describe reality objectively. Let’s break it down:
> - **The Problem of Self-Reference:** Systems that define themselves create a closed loop. Any attempt to challenge the system from the “outside” is immediately dismissed as not belonging to the system. This makes it appear unfalsifiable, as you pointed out. This is a classic problem in logic and epistemology.
> - **Ontology and Epistemology Intertwined:** Ontology deals with the nature of being, reality, and existence. Epistemology deals with the nature of knowledge, justification, and belief. Your question highlights how these two are intertwined. If our system of knowledge (epistemology) is defined in a way that makes it immune to external critique, it raises questions about how accurately it can represent reality (ontology).
> - **The Limits of Formal Systems:** This issue is related to Gödel’s incompleteness theorems, which demonstrate that any sufficiently complex formal system (like mathematics) will contain statements that are true but cannot be proven within the system itself. This suggests inherent limitations in our ability to create complete and consistent systems of knowledge.
> - **The Scientific Method as a Meta-System:** The scientific method, while not a formal system in the same way as mathematics, faces a similar challenge. It’s a meta-system, a set of principles for *generating* knowledge, rather than a body of knowledge itself. While specific scientific theories are falsifiable, the method itself is more like a framework. We can’t “falsify” the framework, but we can evaluate its effectiveness and refine it over time.
> - **Fallibilism:** A key aspect of the scientific method is the acceptance of fallibilism – the idea that all our knowledge is provisional and subject to revision. This acknowledges the inherent limitations of our systems of understanding and emphasizes the importance of continuous testing and refinement.
> - **Pragmatism:** Ultimately, the value of the scientific method lies not in its absolute certainty, but in its practical effectiveness. It has proven to be an incredibly powerful tool for generating knowledge about the universe and developing technologies that improve our lives. This pragmatic justification, while not addressing the ontological issue directly, provides a strong argument for the continued use and refinement of the scientific approach.
> **In short:** The ontological problem you’ve identified is a valid concern. Self-defining systems create a challenge for falsification. However, the scientific method, while not immune to this challenge, addresses it through fallibilism, continuous testing, and a focus on practical effectiveness. It’s a constantly evolving process, and its value lies in its ability to generate increasingly accurate and useful models of reality, even if those models are never considered absolutely complete or final.
